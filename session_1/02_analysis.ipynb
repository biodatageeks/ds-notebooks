{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajęcia 1 (część 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie danych\n",
    "\n",
    "Bedziemy wykorzystywać danye z ankiety StackOverflow z 2020.\n",
    "\n",
    "https://insights.stackoverflow.com/survey/\n",
    "\n",
    "Dane sa dostepne na google drive. Skorzystamy z modułu GoogleDriveDownloader, ktory pozwala pobrac dokument o podanym id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path_dir = str(Path.home()) + \"/data/2020/\"  # ustawmy sciezke na HOME/data/2020\n",
    "archive_dir = path_dir + \"survey.zip\"        # plik zapiszemy pod nazwa survey.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sciagniecie pliku we wskazane miejsce\n",
    "gdd.download_file_from_google_drive(file_id='1dfGerWeWkcyQ9GX9x20rdSGj7WtEpzBB',\n",
    "                                    dest_path=archive_dir,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 1 </span>\n",
    "Zapoznaj sie z plikami tekstowymi (survey_results_public.csv oraz survey_results_schema.csv). Podejrzyj ich zawartość, sprawdź ich wielkość (liczba liniii oraz rozmiar). Wgraj plik do swojego kubełka na GCS do survey/2020/ gs://bdg-lab-$USER/survey/2020/. Jesli nie masz kubełka stwórz go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Podłączenie do sesji Spark na GKE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WAZNE\n",
    "jesli w poprzednim notatniku masz aktywną sesję Spark zakończ ją (w poprzednim notatniku) poleceniem spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark.stop()\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config(\"spark.executor.instances\", \"1\")\\\n",
    ".config('spark.driver.memory','1g')\\\n",
    ".config('spark.executor.memory', '1g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostęp do danych na GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "print(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ścieżka dostępu do pliku na GCS\n",
    "gs_path = f'gs://bdg-lab-{user_name}/survey/2020/survey_results_public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "Platforma Apache Spark posiada komponent Spark SQL, który pozwala traktować dane jak tabele w bazie danych. Można zakładać swoje schematy baz danych oraz korzystać z języka SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"survey_2020\"                               # nazwa tabeli ktora bedziemy chcieli stworzyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')       # usun te tabele jesli istniala wczesniej \n",
    "\n",
    "# stworz tabele korzystajac z danych we wskazanej lokalizacji\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true) \\\n",
    "          LOCATION \"{gs_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weryfikacja danych \n",
    "Sprawdzmy zaczytane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"describe {table_name}\").show() # nie wszystkie dane ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"describe {table_name}\").show(100, truncate=False) # niepoprawne typy danych... \"NA\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsługa wartosci 'NA' - ponowne stworzenie tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "# wykorzystujemy dodatkową opcję: NULLVALUE \"NA\"\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE {table_name}\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT  Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzenie liczności tabeli\n",
    "spark.sql(f\"select count(*) from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select count(*) from {table_name}\").explain()  # tak jak na poprzednich zajeciach mozemy wygenerowac plany wykonania polecenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podgląd danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteka Pandas\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Moduł Pandas jest biblioteką Pythonową do manipulacji danymi. W szczegolnosci w pandas mozemy stworzyc ramki danych i wykonywac na niej analize, agregacje oraz wizualizacje danych. \n",
    "Przy nieduzych zbiorach danych i prostych operacjach to doskonała biblioteka. Jednak kiedy zbior danych sie rozrasta lub kiedy wymagane sa zlozone transformacje to operacje moga byc wolne.\n",
    "\n",
    "Operacje na rozproszonych danych sa szybsze. Ale tu takze napotykamy ograniczenia np trudność w wizualizacji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_name} limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ważne** \n",
    "\n",
    "Metoda toPandas() na ramce pyspark, konwertuje ramkę pyspark do ramki pandas. Wykonuje akcje pobrania (collect) wszystkich danych z executorów (z JVM) i transfer do  programu sterujacego (driver) i konwersje do typu Pythonowego w notatniku. Ze względu na ograniczenia pamięciowe w programie sterującym należy to wykonywać na podzbiorach danych.\n",
    "\n",
    "**DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side.**\n",
    "\n",
    "**Note that DataFrame.toPandas() results in the collection of all records in the DataFrame to the driver program and should be done on a small subset of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = spark.sql(f\"select * from {table_name} LIMIT 10\")\n",
    "local_df = spark.sql(f\"select * from {table_name} LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist_df)  # dataframe Sparkowy (\"przepis na dane, rozproszony, leniwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(local_df)  # dataframe Pandasowy (lokalny, sciągnięty do pamięci operacyjnej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)    # pokazuj wszystkie kolumny\n",
    "# pd.reset_option(“display.max_columns”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 2 </span>\n",
    "Napisz w Spark SQL zapytanie które zwróci średnią liczbę godzin przepracowywanych przez z respondentów pogrupowanych ze względu na kraj. Następnie przekształć wynik do ramki pandasowej i ją wyświetl.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacje\n",
    "\n",
    "Do wizualizacji będziemy się posługiwać modułami matplotlib (https://matplotlib.org/) i seaborn (https://seaborn.pydata.org/). Do bardzo rozbudowane moduły, zachęcamy do eksploracji oficjalnych dokumentacji. Na zajęciach zrealizujemy następujące wykresy:\n",
    "* histogramy\n",
    "* liniowe \n",
    "* wiolinowe\n",
    "* kołowe \n",
    "\n",
    "Moduły wizualizacyjne wymagają danych na lokalnej maszynie. Mogą być to natywne typy danych Pythonowe (słowniki, listy) ale także np ramki danych pandasowe. ~~Nie działa wizualizacja na ramkach danych Sparkowych.~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj histogram wieku respondentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych\n",
    "# przycinamy dane tylko do zakresu ktory jest potrzebny do realizacji polecenia\n",
    "ages = spark.sql(f\"SELECT CAST (Age AS INT) \\\n",
    "                    FROM {table_name} \\\n",
    "                    WHERE age IS NOT NULL \\\n",
    "                    AND age BETWEEN 10 AND 80\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages.hist(\"Age\", bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(ages, bins=10, rug=True, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaki jest udział programistów hobbistów? Narysuj wykres kołowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będzie nas interesowała ta proporcja ze względu na płeć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie i zliczenie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "hobby_all = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL GROUP BY Hobbyist\").toPandas()\n",
    "hobby_men = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Man' GROUP BY Hobbyist\").toPandas()\n",
    "hobby_women = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Woman' GROUP BY Hobbyist\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_women.plot.pie(y='cnt', labels=hobby_women['Hobbyist'], title=\"Women\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", ax=axes[0], autopct='%.0f')\n",
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", ax=axes[1], autopct='%.0f')\n",
    "hobby_women.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Women\", ax=axes[2], autopct='%.0f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres liniowy. Zależność między wiekiem a liczbą przepracowanych godzin\n",
    "Interesują nas dla developerzy profesjonaliści (nie hobbiści) w przedziale wiekowym 18-65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "age_work = spark.sql(f\"SELECT age, CAST (avg(WorkWeekHrs) AS INT) AS avg FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs IS NOT NULL AND age BETWEEN 18 AND 65 AND hobbyist = 'No' \\\n",
    "            GROUP BY age \\\n",
    "            ORDER BY age ASC\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work.plot(x='age', y='avg', kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"age\", y=\"avg\", kind=\"line\", data=age_work);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres słupkowy. Pokaż liczbę respondentów na kraj\n",
    "\n",
    "Interesuje nas tylko 10 krajów o najwyższej liczbie respondentow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (grupowanie, zliczenie, sortowanie oraz przyciecie do 10 wyników) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "max_countries = spark.sql(f\"SELECT country, COUNT(*) AS cnt \\\n",
    "                FROM {table_name} \\\n",
    "                GROUP BY country \\\n",
    "                ORDER BY cnt DESC \\\n",
    "                LIMIT 10 \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_countries.plot.bar(y='cnt', x='country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"cnt\", kind=\"bar\",\\\n",
    "            data=max_countries).set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres słupkowy. Średnie zarobki w  krajach w ktorych jest powyzej 1000 respondentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej, filtrowanie po liczności i grup oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_salary = spark.sql(f\"SELECT country, \\\n",
    "    CAST (avg(ConvertedComp) AS INT) as avg \\\n",
    "    FROM {table_name} \\\n",
    "    WHERE country IS NOT NULL \\\n",
    "    GROUP BY country \\\n",
    "    HAVING COUNT(*) > 1000 \\\n",
    "    ORDER BY avg DESC \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_salary.plot.barh((\"country\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot. Pokaz rozklad pensji w krajach gdzie jest powyzej 1000 respondentów\n",
    "Tutaj będziemy musieli skorzystać z podzapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych na rozproszonych danych (spark sql). Mamy tu do czynienia z podzapytaniem\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_comp = spark.sql(f\"SELECT country, CAST(ConvertedComp AS INT) \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE country IN (SELECT country FROM {table_name} GROUP BY country HAVING COUNT(*) > 1000) \\\n",
    "                AND ConvertedComp IS NOT NULL AND ConvertedComp > 0 \\\n",
    "                ORDER BY ConvertedComp desc\").toPandas()\n",
    "country_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_comp.boxplot(column=\"ConvertedComp\", by=\"country\", \\\n",
    "                     showfliers=False, rot=60, meanline=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"ConvertedComp\", kind=\"box\", \\\n",
    "            showfliers=False, data=country_comp, palette=\"Blues\")\\\n",
    "    .set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 3 </span>\n",
    "Narysuj rozklad pensji w zaleznosci od plci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow programowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Języki programowania są zapisane w pojedynczej komórce. Będzie trzeba je rozdzielić i zliczyć. Tak przygotowane dane dopiero posłużą nam do narysowania wykresu. Wykorzystamy funkcję `posexplode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_pd = langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").toPandas()\n",
    "langs_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=langs_pd[\"count\"], y=langs_pd[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow wsrod Data Scientists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy sobie funkcję, która przekształca nam języki do wymaganej przez nas postaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def prepare_lang(df, colName='LanguageWorkedWith'):\n",
    "    summary = df.select(posexplode(split(colName, \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_ds = spark.sql(f\"SELECT LanguageWorkedWith \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE DevType LIKE '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(langs_ds).toPandas()\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres którego chcą wykorzystywać w przyszłości Data Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_desired = spark.sql(f\"select LanguageDesireNextYear \\\n",
    "                from {table_name} \\\n",
    "                where DevType like '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(lang_desired, 'LanguageDesireNextYear').toPandas()\n",
    "\n",
    "figure(num=None, figsize=(8, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres prezentujący liczbę godzin na pracy w zależności od wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select distinct EdLevel from {table_name}\").show(truncate=False) # jakie są wartości wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ed_pandas = spark.sql(f\"SELECT EdLevel, WorkWeekHrs FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs BETWEEN 10 AND 80 \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Bachelor’s degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Master’s degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"WorkWeekHrs\", data=ed_pandas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres wiolinowy pokazujacy rozkład dochodów w zależności od wykształcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_pay = spark.sql(f\"SELECT EdLevel, CAST (CompTotal AS INT) AS CompTotal FROM {table_name} \\\n",
    "            WHERE CompTotal BETWEEN 0 AND 1000000  \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Bachelor’s degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Master’s degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"CompTotal\", kind=\"boxen\",\n",
    "            data=ed_pay);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⭐ Narysuj heatmape odwiedzin na StackOverflow dla wybranych krajów ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT SOVisitFreq FROM {table_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_v = spark.sql(f\"SELECT SOVisitFreq, t1.country, COUNT(*)/first(t2.t) AS cnt from {table_name} t1 \\\n",
    "            JOIN (SELECT country, COUNT(*) as t FROM {table_name} GROUP BY country) t2 \\\n",
    "            ON t1.country = t2.country \\\n",
    "            WHERE t1.country IS NOT NULL AND SOVisitFreq IS NOT NULL \\\n",
    "            AND t1.country IN ('Poland', 'United States', 'Russian Federation', 'China', 'India', 'Germany', 'Japan') \\\n",
    "            GROUP BY t1.country, SOVisitFreq\").toPandas()\n",
    "\n",
    "so_v['SOVisitFreq'] = pd.Categorical(so_v['SOVisitFreq'], [\"I have never visited Stack Overflow (before today)\", \"Less than once per month or monthly\", \"A few times per month or weekly\", \"A few times per week\", \"Daily or almost daily\", \"Multiple times per day\"])\n",
    "# so_v.sort_values['SOVisitFreq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap2_data = pd.pivot_table(so_v, values='cnt', index=['country'], columns='SOVisitFreq')\n",
    "sns.heatmap(heatmap2_data, cmap=\"BuGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 4 </span>\n",
    "* Narysuj wykres słupkowy popularności wykorzystywanych baz danych przez profesjonalnych programistów.\n",
    "* Narysuj wykres kołowy przedstawiający procentowy udział poziomu wykształcenia inz, mgr i dr w grupie respondentów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
