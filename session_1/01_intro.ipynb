{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajęcia 1 (część 1)\n",
    "\n",
    "Zakres:\n",
    "* Zapoznanie się ze srodowiskiem pracy (Dataproc na Google Cloud Platform)\n",
    "* Operacje na obiektowym systemie plików (GCS)\n",
    "* Interaktywna analiza rozproszonych danych przy wykorzystaniu modułu Spark Core i Spark SQL\n",
    "* Wprowadzenie modułu pandas\n",
    "* Wizualizacje przy wykorzystaniu matplotlib oraz seaborn\n",
    "\n",
    "Języki:\n",
    "* bash, Python, SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krótka lista przydatnych poleceń bash: https://www.reddit.com/r/linux/comments/9rns12/some_linux_commands_cheatsheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook\n",
    "\n",
    "* Interaktywny notatnik dostepny poprzez przeglądarkę.\n",
    "* Służy do wpisywania poleceń w wybranych jezykach programowania oraz opis w tzw jezyku markdown.\n",
    "* Polecenia/opis wpisujemy do komórek. Komórki są wykonywane przez tzw. kernel (np Python w określonej wersji)\n",
    "* Działa w tryb edycji komórek + tryb wykonywania poleceń. (przejście poprzez ESC i ENTER)\n",
    "* Podstawowe skróty klawiaturowe: \n",
    "  * Esc/Enter\n",
    "  * strzalki\n",
    "  * ctrl-enter/shift-enter\n",
    "  * a/b/d\n",
    "  * y/m \n",
    "* Moze wykonywać polecenia powłoki (bash) !  %%bash\n",
    "* Notatnik zapisywany jest w formacie ipynb (IPythonNoteBook)\n",
    "* Kolejnosc uruchomienia komórek moze byc rozna.\n",
    "* Mozliwosc zatrzymania kernela i uruchomienia od nowa. \n",
    "\n",
    "Polecamy film: https://www.youtube.com/watch?v=HW29067qVWk\n",
    "\n",
    "Przykładowe komórki Pythonowe i Bashowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user = os.environ.get('USER')\n",
    "msg = f\"Hello {user}!\"\n",
    "print (msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd # pokaż aktualną scieżkę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root/ds-notebooks/session_1/\n",
    "head 01_intro.ipynb\n",
    "echo \"--------------------\"\n",
    "tail 01_intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kopiowanie danych do swojego katalogu domowego\n",
    "\n",
    "W katalogu `session1` znajduje sie plik który bedzie nam dzisiaj słuzyl do pracy.  \n",
    "Dane pochodzą z https://gdac.broadinstitute.org/ i zawierają dane z badaniami nad mutacjami typu CNV w obrebie genu BRCA2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root/ds-notebooks/session_1/\n",
    "mkdir -p ~/data     # stworz katalog data (i posrednie)\n",
    "cp brca.txt ~/data  # skopiuj plik z lokalizacji do bieżącej lokalizacji\n",
    "ls -lah   ~/data       # wylistuj zawartość bieżącego katalogu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ~/data/brca.txt   # pokaż początek pliku\n",
    "echo                   # pusta linia\n",
    "tail ~/data/brca.txt   # pokaż koniec pliku \n",
    "echo\n",
    "wc -l ~/data/brca.txt  # policz linie pliku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Środowisko Google Cloud Dataproc\n",
    "\n",
    "### TODO Dataproc intro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Wykorzystanie Google Cloud Storage\n",
    "\n",
    "#### Projekt\n",
    "* Wszystkie dane przynależą do konkretnego projektu.\n",
    "* Do projektu mogą mieć dostęp użytkownicy. \n",
    "* Projekt ma zdefiniowane metody uwierzytelniające, rozliczenia, monitorowanie etc. \n",
    "#### Kubełek (bucket)\n",
    "* Kubełek (bucket) to kontener na pliki/obiekty. \n",
    "* Nazwa Bucketu musi być unikalna w skali całej usługi u wszystkich użytkowników (!) \n",
    "* Kubełków nie można zagnieżdzać\n",
    "* W kubełkach możemy tworzyć foldery i tam logicznie grupować pliki.\n",
    "* Kubełek wraz z zawartością może zostać udostępniony publicznie.\n",
    "* Kubełkowi nie można zmienić nazwy lub metadanych. Trzeba go usunąć i stworzyć ponownie.\n",
    "#### Obiekt \n",
    "* obiekty przechowywane w kubełkach \n",
    "* obiekty mają zawartość oraz metadane\n",
    "* obiekty są niemodyfikowalne \n",
    "\n",
    "Do operacji na Google Storage można wykorzystać narzędzie `gsutil`: \n",
    "\n",
    "##### Operacje na kubełkach\n",
    "* listowanie kubełków (buckets) - `ls`\n",
    "* tworzenie nowego kubełka - `mb`\n",
    "* usuwania kubełka - `rm`\n",
    "* listowanie zawartości kubełków - `ls` \n",
    "* udostępnianie - `iam`\n",
    "\n",
    "##### Operacje na obiektach\n",
    "* dodawania pliku do kubełka - `cp`\n",
    "* kopiowanie - `cp`\n",
    "* usuniecie z kubełka - `cp`\n",
    "* pobranie informacji o obiekcie - `stat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operacje na kubełkach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $USER  # wyswietlenie nazwy uzytkownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Please update user id\n",
    "%env USER_ID=9903\n",
    "%env SEMESTER=2024l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb gs://big-data-lab-$SEMESTER-$USER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://big-data-lab-$SEMESTER-$USER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil du -s gs://big-data-lab-$SEMESTER-$USER_ID # ile zajmuje przestrzeni?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil du -L -b gs://big-data-lab-$SEMESTER-$USER_ID # listowanie zawartości "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operacje na zawartości kubełków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r gs://big-data-lab-$SEMESTER-$USER_ID  # listowanie zawartosci kubełka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ~/ds-notebooks/README.md gs://big-data-lab-$SEMESTER-$USER_ID  # upload obiektu do kubełka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r gs://big-data-lab-$SEMESTER-$USER_ID  # listowanie zawartosci kubełka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam get gs://big-data-lab-$SEMESTER-$USER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil stat gs://big-data-lab-$SEMESTER-$USER_ID/README.md # metadane obiektu w kubełku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przeniesienie do docelowego kubełka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 1 </span>\n",
    "Wgraj plik brca.txt do kubełka `gs://ds-$SEMESTER-$USER_ID-notebook-data` do katalogu `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 2 </span>\n",
    "a) Utwórz plik zawierający aktualną datę i godzinę.  \n",
    "b) Wrzuć plik do kubełka `gs://big-data-lab-$SEMESTER-$USER_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie pliku z powrotem na lokalny dysk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp gs://ds-$SEMESTER-$USER_ID-notebook-data/data/brca.txt .  # gdzie sie zapisal ten plik?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "* Apache Spark - platforma ogólnego zastosowania, open source, do przetwarzania duzych zbiorów danych.\n",
    "* Posiada API dla języków programowania: Scala, Python i R. \n",
    "* Przetwarzanie w Spark przetwarzanie jest wykonywane w większości  wprost w pamięci operacyjnej.\n",
    "* Przeznaczenie: do uruchamiania  aplikacji i skryptów z wykorzystaniem uczenia maszynowego lub interaktywnych kwerend.\n",
    "* Spark ten wspiera SQL (typ DataFrames), przetwarzanie strumieniowe oraz przetwarzanie grafów.\n",
    "* Integracja z lokalną pamięci masową, rozproszonymi lub obiektowymi systemu plików.\n",
    "* Spark można uruchamić na pojedynczej maszynie na środowisku klastrowym, lub w chmurze. \n",
    "\n",
    "Zakres na dzisiejsze laboratorium:\n",
    "* stworzenie sesji Spark\n",
    "* zaczytanie danych z pliku tekstowego\n",
    "* kwerendy na danych\n",
    "\n",
    "Będziemy korzystać z pliku ktory zapisalismy na GCS w pierwszej części ćwiczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r gs://ds-$SEMESTER-$USER_ID-notebook-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie sesji Sparkowej\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.executor.instances\", \"1\") \\\n",
    ".config(\"spark.executor.memory\", \"1g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.executor.instances\", \"1\") \\\n",
    ".config(\"spark.executor.memory\", \"1g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ręczne utworzenie zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stworzenie rozproszonego DF na podstawie podanej listy krotek \n",
    "df_users = spark.createDataFrame(\n",
    "    [(123, 'Jan Kowalski', 'jan@kowalski.pl'), \n",
    "     (124, 'Anna Nowak', 'anna@nowak.pl'), \n",
    "     (125, 'Janusz Kowalski', 'janusz@kowalski.pl'), \n",
    "     (126, 'Anita Nowak', 'anita@nowak.pl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = spark.createDataFrame(\n",
    "    [(123, 'Jan Kowalski', 'jan@kowalski.pl'), \n",
    "     (124, 'Anna Nowak', 'anna@nowak.pl'), \n",
    "     (125, 'Janusz Kowalski', 'janusz@kowalski.pl'), \n",
    "     (126, 'Anita Nowak', 'anita@nowak.pl')],\n",
    "    schema='id int not null, name string, email string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Wczytanie danych\n",
    "Zazwyczaj dane mamy przechowane na zewnątrz naszego notatnika (np. w pliku) i należy je odczytać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Please update user id\n",
    "semester = '2024l'\n",
    "user_id = '9903'\n",
    "input_path = f'gs://ds-{semester}-{user_id}-notebook-data/data/brca.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(input_path, format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charakterystyka danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (df)                 # jaki jest typ danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()              # fizyczny plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)          # logiczny i fizyczny plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions() # liczba partycji (bloków danych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()          # schemat danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()                # wymiary (liczba wierszy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)           # wymiary (liczba kolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()      # pokaż podsumowanie danych w tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(\"Chromosome\").show() # pokaż podsumowanie danych w kolumnie "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odczyt danych\n",
    "Z wykorzystaniem API DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()                    # pokaż 20 wierszy danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, truncate=False) # pokaż 10 wierszy, nie skracaj danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Sample\").show()  # pokaż tylko kolumne sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Chromosome\", \"Start\", \"End\").show() # pokaż kolumny chromosome, start i end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrom = df.select(df.Chromosome).distinct() # stworz nowy DF zawierajacy tylko unikalne wartosci chromosomow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrom.count()                     # policz wiersze w nowym DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21\").show() # pokaz dane spelniajace warunek (przy uzyciu filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"Chromosome > 21\").show() # pokaz dane spelniajace warunek (przy uzyciu when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21\").explain()  # pokaz plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21 and Segment_Mean > 0\").show() # pokaz dane spelniajace warunki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grupowanie i funkcje agregujące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Chromosome\").count().show()   # dokonaj grupowania po chromosomie i policz rekordy w grupie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Chromosome\").avg(\"Segment_Mean\").show() # dokonaj grupowania po chromosomie i policz srednia wartosc segment_mean w grupie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pogrupuj dane wzgledem probki i chromosomu, policz rekordy w grupie\n",
    "df.groupBy(\"Sample\",\"Chromosome\").count().orderBy(asc(\"Sample\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolumny wyliczane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Length\", col(\"End\") - col(\"Start\")).show() # dodaj kolumne dlugosc jako end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Material\", lit(\"DNA\")).show()     # dodaj kolumne o stalej wartosci 'DNA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Chromosome2\", concat(lit(\"chr\"), col(\"Chromosome\"))).show() # dodaj kolumne z konkatencja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Chromosome\").show()  # usun kolumne Chromosome. \n",
    "# Czy DF została pozbawiona kolumny na trwale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapis wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn (\"Chromosome2\", concat(lit(\"chr\"), col(\"Chromosome\")))  # dodaj kolumne z konkatenacja i zapisz do nowego DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'gs://ds-{semester}-{user_id}-notebook-data/data/brca2.txt'\n",
    "df2.write.format(\"csv\").mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://ds-$SEMESTER-$USER_ID-notebook-data/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat gs://ds-$SEMESTER-$USER_ID-notebook-data/data/brca.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wazne!\n",
    "Na koniec pracy przy tym notatniku (po wykonaniu poniższego polecenia) koniecznie zatrzymaj sesję Sparkową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 3</span>\n",
    "\n",
    "a) Ile jest unikalnych danych próbek w tym zbiorze danych?\n",
    "\n",
    "b) Pokaż nazwę próbki, numer chromosomu, początek i koniec segmentu dla segmentów występujących na chromosomie 21,22,23 i mających startową pozycję większą niż 10000000. \n",
    "\n",
    "c) Ile występuje wierszy dla każdej z próbki? Pokaż wynik z pełną nazwą próbki. Zapisz wynik do pliku na GCS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
