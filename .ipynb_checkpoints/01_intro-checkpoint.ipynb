{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajęcia 1 (część 1)\n",
    "\n",
    "Zakres:\n",
    "* Zapoznanie sie ze srodowiskiem pracy (GKE na GCP)\n",
    "* Operacje na obiektowym systemie plików (GCS)\n",
    "* Interaktywna analiza rozproszonych danych przy wykorzystaniu modułu Spark Core i Spark SQL\n",
    "* Wprowadzenie modułu pandas\n",
    "* Wizualizacje przy wykorzystaniu matplotlib oraz seaborn\n",
    "\n",
    "Języki:\n",
    "* bash, Python, SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krótka lista przydatnych poleceń bash: https://www.reddit.com/r/linux/comments/9rns12/some_linux_commands_cheatsheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook\n",
    "\n",
    "* Interaktywny notatnik dostepny poprzez przeglądarkę.\n",
    "* Służy do wpisywania poleceń w wybranych jezykach programowania oraz opis w tzw jezyku markdown.\n",
    "* Polecenia/opis wpisujemy do komórek. Komórki są wykonywane przez tzw. kernel (np Python w określonej wersji)\n",
    "* Działa w tryb edycji komórek + tryb wykonywania poleceń. (przejście poprzez ESC i ENTER)\n",
    "* Podstawowe skróty klawiaturowe: \n",
    "  * Esc/Enter\n",
    "  * strzalki\n",
    "  * ctrl-enter/shift-enter\n",
    "  * a/b/d\n",
    "  * y/m \n",
    "* Moze wykonywać polecenia powłoki (bash) !  %%bash\n",
    "* Notatnik zapisywany jest w formacie ipynb (IPythonNoteBook)\n",
    "* Kolejnosc uruchomienia komórek moze byc rozna.\n",
    "* Mozliwosc zatrzymania kernela i uruchomienia od nowa. \n",
    "\n",
    "Polecamy film: https://www.youtube.com/watch?v=HW29067qVWk\n",
    "\n",
    "Przykładowe komórki Pythonowe i Bashowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user = os.environ.get('USER')\n",
    "msg = f\"Hello {user}!\"\n",
    "print (msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd # pokaż aktualną scieżkę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head 01_intro.ipynb\n",
    "echo \"--------------------\"\n",
    "tail 01_intro.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kopiowanie danych do swojego katalogu domowego\n",
    "\n",
    "W katalogu `/mnt/data/datascience` znajduje sie plik który bedzie nam dzisiaj słuzyl do pracy. Należy go skopiować do swojego katalogu domowego. \n",
    "Dane pochodzą z https://gdac.broadinstitute.org/ i zawierają dane z badaniami nad mutacjami typu CNV w obrebie genu BRAC2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ~/data     # stworz katalog data (i posrednie)\n",
    "cp /mnt/data/datascience/brca.txt ~/data  # skopiuj plik z lokalizacji do bieżącej lokalizacji\n",
    "ls -lah   ~/data       # wylistuj zawartość bieżącego katalogu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head ~/data/brca.txt   # pokaż początek pliku\n",
    "echo                   # pusta linia\n",
    "tail ~/data/brca.txt   # pokaż koniec pliku \n",
    "echo\n",
    "wc -l ~/data/brca.txt  # policz linie pliku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Środowisko Google Kubernetes Engine \n",
    "\n",
    "Logowanie kontem github. \n",
    "\n",
    "🙏  Podziękowania dla Operatora Chmury Krajowej (https://chmurakrajowa.pl/) i Google Poland za udostępnienie infrastruktury i pokrycie kosztów wykorzystania zasobów podczas zajęć. 🙏 \n",
    "\n",
    "\n",
    "## Kubernetes\n",
    "\n",
    "Kubernetes (k8s) to otwarta platforma do koordynacji wysoko dostępnego klastra. \n",
    "* Umożliwia  deklaratywną konfigurację, automatyzację wdrażania, skalowanie i autoskalowanie rozwiązań.\n",
    "* Pozwala uruchamiać aplikacje/narzędzie bez przypisywania ich do konkretnej maszyny. Aplikacje muszą być niezależne od konkretnego serwera: muszą być skonteneryzowane.\n",
    "* Możliwe do uruchomienia na prywatnym centrum danych, infrastrukturze hybrydowej lub chmurze publicznej.\n",
    "\n",
    "\n",
    "**Klaster Kubernetes**  \n",
    "* **Węzeł sterujący (Master node)** koordynuje działanie klastra np. zlecanie uruchomienia aplikacji, utrzymywanie pożądanego stanu aplikacji, skalowanie aplikacji i instalowanie nowych wersji\n",
    "* **Węzeł roboczy (Worker node)** Na węzłach uruchamiane są aplikacje. Na każdym węźle działa agent zarządzający tym węzłem i komunikujący się z masterem Kubernetes (API). Węzeł zawiera także narzędzia do obsługi kontenerów\n",
    "\n",
    "**Pod** to grupa jednego/wielu kontenerów wraz ze wspólnymi zasobami (np. dysk). Pod tworzy \"wirtualney serwer\" i może zawierać różne kontenery aplikacji współdzielące zasoby i kontekst wykonawczy na tym samym węźle.\n",
    "* Pod jest uruchamiany na węźle roboczym. Węzeł jest maszyną roboczą, fizyczną lub wirtualną, w zależności od klastra. \n",
    "* Węzeł może zawierać wiele podów. \n",
    "* Kubernetes master automatycznie zleca uruchomienie podów na różnych węzłach w ramach klastra. Automatyczne zlecanie uruchomienia bierze pod uwagę zasoby dostępne na każdym z węzłów.\n",
    "\n",
    "![](../img/gke.png) \n",
    "\n",
    "\n",
    "## Wykorzystanie Google Cloud Storage\n",
    "\n",
    "#### Projekt\n",
    "* Wszystki dane przynależą do konkretnego projektu.\n",
    "* Do projektu mogą mieć dostęp użytkownicy. \n",
    "* Projekt ma zdefiniowane metody uwierzytelniające, rozliczenia, monitorowanie etc. \n",
    "#### Kubełek (bucket)\n",
    "* Kubełek (buckket) to kontener na pliki/obiekty. \n",
    "* Nazwa Bucketu musi być unikalna w skali całej usługi u wszystkich użytkowników (!) \n",
    "* Kubełków nie można zagnieżdzać\n",
    "* W kubełkach możemy tworzyć foldery i tam logicznie grupować pliki.\n",
    "* Kubełek wraz z zawartością może zostać udostępniony publicznie.\n",
    "* Kubełkowi nie można zmienić nazwy lub metadanych. Trzeba go usunąć i stworzyć ponownie.\n",
    "#### Obiekt \n",
    "* obiekty przechowywane w kubełkach \n",
    "* obiekty mają zawartość oraz metadane\n",
    "* obiekty są niemodyfikowalne \n",
    "\n",
    "Do operacji na Google Storage można wykorzystać narzędzie `gsutil`: \n",
    "\n",
    "##### Operacje na kubełkach\n",
    "* listowanie kubełków (buckets) - `ls`\n",
    "* tworzenie nowego kubełka - `mb`\n",
    "* usuwania kubełka - `rm`\n",
    "* listowanie zawartości kubełków - `ls` \n",
    "* udostępnianie - `iam`\n",
    "\n",
    "##### Operacje na obiektach\n",
    "* dodawania pliku do kubełka - `cp`\n",
    "* kopiowanie - `cp`\n",
    "* usuniecie z kubełka - `cp`\n",
    "* pobranie informacji o obiekcie - `stat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operacje na kubełkach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo $USER  # wyswietlenie nazwy uzytkownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb gs://bdg-lab-$USER  # stworzenie bucketu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://bdg-lab-$USER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil du -s  gs://bdg-lab-$USER  # ile zajmuje przestrzeni?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -L -b gs://bdg-lab-$USER  # listowanie zawartości "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operacje na zawartości kubełków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r gs://bdg-lab-$USER  # listowanie zawartosci kubełka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp ~/work/git/ds-notebooks/README.md gs://bdg-lab-$USER  # upload obiektu do kubełka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r gs://bdg-lab-$USER # listowanie zawartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam get gs://bdg-lab-$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil stat gs://bdg-lab-$USER/README.md # metadane obiektu w kubełku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam ch allUsers:objectViewer gs://bdg-lab-$USER # dodanie uprawnien do odczytu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po wykonaniu tego polecenia nasz kubełek staje się publiczny i możemy się do niego \n",
    "http://storage.googleapis.com/bucket-NAZWA_UZYTKOWNIKA. Zawartość pliku można odczytać poprzez http://storage.googleapis.com/bucket-NAZWA_UZYTKOWNIKA/NAZWA_OBIEKTU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam ch -d allUsers:objectViewer gs://bdg-lab-$USER # usuniecie uprawnien do odczytu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz można ponownie zweryfikowac możliwość publicznego odczytu danych z kubełka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przeniesienie do docelowego kubełka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 1 </span>\n",
    "Wgraj plik brca.txt do kubełka `gs://bdg-lab-$USER`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 2 </span>\n",
    "a) Utwórz plik zawierający aktualną date i godzinę. \n",
    "b) Udostępnij plik publicznie na google storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie pliku z powrotem na lokalny dysk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp gs://bdg-lab-$USER/brca.txt .  # gdzie sie zapisal ten plik?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "* Apache Spark platforma ogólnego zastosowania, opensource, do przetwarzania duzych zbiorow danych.\n",
    "* Posiada  API dla języków programowania: Scala, Python i R. \n",
    "* Przetwarzanie w Spark przetwarzanie jest wykonywane w większości  wprost w pamięci operacyjnej.\n",
    "* Przeznaczenie: do uruchamiania  aplikacji i skryptów z wykorzystaniem uczenia maszynowego lub interaktywnych kwerend.\n",
    "* Spark ten wspiera SQL (typ DataFrames), przetwarzanie strumieniowe oraz przetwarzanie grafów.\n",
    "* Integracja z lokalną pamięci masową, rozproszonymi lub obiektowymi systemu plików.\n",
    "* Spark można uruchamić na pojedynczej maszynie na środowisku klastrowym, lub w chmurze. \n",
    "\n",
    "Zakres na dzisiejsze laboratorium:\n",
    "* stworzenie sesji Spark\n",
    "* zaczytanie danych z pliku tekstowego\n",
    "* kwerendy na danych\n",
    "\n",
    "Bedziemy korzystac z pliku ktory zapisalismy na GCS w pierwszej czesci cwiczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -r gs://bdg-lab-$USER/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie sesji Sparkowej\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.executor.instances\", \"1\") \\\n",
    ".config(\"spark.executor.memory\", \"1g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.executor.instances\", \"1\") \\\n",
    ".config(\"spark.executor.memory\", \"1g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ręczne utworzenie zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stworzenie rozproszonego DF na podstawie podanej listy krotek \n",
    "df_users = spark.createDataFrame(\n",
    "    [(123, 'Jan Kowalski', 'jan@kowalski.pl'), \n",
    "     (124, 'Anna Nowak', 'anna@nowak.pl'), \n",
    "     (125, 'Janusz Kowalski', 'janusz@kowalski.pl'), \n",
    "     (126, 'Anita Nowak', 'anita@nowak.pl')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = spark.createDataFrame(\n",
    "    [(123, 'Jan Kowalski', 'jan@kowalski.pl'), \n",
    "     (124, 'Anna Nowak', 'anna@nowak.pl'), \n",
    "     (125, 'Janusz Kowalski', 'janusz@kowalski.pl'), \n",
    "     (126, 'Anita Nowak', 'anita@nowak.pl')],\n",
    "    schema='id int not null, name string, email string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Wczytanie danych\n",
    "Zazwyczaj dane mamy przechowane na zewnątrz naszego notatnika (np. w pliku) i należy je odczytać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "print(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f'gs://bdg-lab-{user_name}/brca.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(input_path, format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charakterystyka danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (df)                 # jaki jest typ danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()              # fizyczny plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)          # logiczny i fizyczny plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions() # liczba partycji (bloków danych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()          # schemat danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()                # wymiary (liczba wierszy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns)           # wymiary (liczba kolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()      # pokaż podsumowanie danych w tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(\"Chromosome\").show() # pokaż podsumowanie danych w kolumnie "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odczyt danych\n",
    "Z wykorzystaniem API DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()                    # pokaż 20 wierszy danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, truncate=False) # pokaż 10 wierszy, nie skracaj danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Sample\").show()  # pokaż tylko kolumne sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Chromosome\", \"Start\", \"End\").show() # pokaż kolumny chromosome, start i end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrom = df.select(df.Chromosome).distinct() # stworz nowy DF zawierajacy tylko unikalne wartosci chromosomow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrom.count()                     # policz wiersze w nowym DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21\").show() # pokaz dane spelniajace warunek (przy uzyciu filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"Chromosome > 21\").show() # pokaz dane spelniajace warunek (przy uzyciu when)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21\").explain()  # pokaz plan wykonania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Chromosome > 21 and Segment_Mean > 0\").show() # pokaz dane spelniajace warunki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grupowanie i funkcje agregujące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Chromosome\").count().show()   # dokonaj grupowania po chromosomie i policz rekordy w grupie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Chromosome\").avg(\"Segment_Mean\").show() # dokonaj grupowania po chromosomie i policz srednia wartosc segment_mean w grupie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pogrupuj dane wzgledem probki i chromosomu, policz rekordy w grupie\n",
    "df.groupBy(\"Sample\",\"Chromosome\").count().orderBy(asc(\"Sample\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolumny wyliczane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Length\", col(\"End\") - col(\"Start\")).show() # dodaj kolumne dlugosc jako end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Material\", lit(\"DNA\")).show()     # dodaj kolumne o stalej wartosci 'DNA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn (\"Chromosome2\", concat(lit(\"chr\"), col(\"Chromosome\"))).show() # dodaj kolumne z konkatencja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Chromosome\").show()  # usun kolumne Chromosome. \n",
    "# Czy DF została pozbawiona kolumny na trwale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapis wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn (\"Chromosome2\", concat(lit(\"chr\"), col(\"Chromosome\")))  # dodaj kolumne z konkatenacja i zapisz do nowego DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'gs://bdg-lab-{user_name}/brca2.txt'\n",
    "df2.write.format(\"csv\").mode(\"overwrite\").save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://bdg-lab-$USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat gs://bdg-lab-$USER/brca.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 3</span>\n",
    "\n",
    "a) Ile jest unikalnych danych próbek w tym zbiorze danych?\n",
    "\n",
    "b) Pokaż nazwę próbki, numer chromosomu, początek i koniec segmentu dla segmentów występujących na chromosomie 21,22,23 i mających startową pozycję większą niż 10000000. \n",
    "\n",
    "c) Ile występuje wierszy dla każdej z próbki? Pokaż wynik z pełną nazwą próbki. Zapisz wynik do pliku na GCS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
