{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zajƒôcia 2\n",
    "\n",
    "## Podsumowanie poprzednich zajƒôƒá:\n",
    "* Zapoznanie siƒô z dwoma ≈õrodowiskami obliczeniowymi (klaster oraz chmura Google). \n",
    "* Operacje na rozproszonym systemie plik√≥w (HDFS)\n",
    "* Operacje na obiektowym systemie plik√≥w (GCS)\n",
    "* Operacje na rozproszonych danych przy wykorzystaniu Apache Spark (API DataFrame)\n",
    "\n",
    "## Zakres dzisiejszych zajƒôƒá:\n",
    "* [ASZ] Analiza rozproszonych danych przy wykorzystaniu modu≈Çu Spark SQL\n",
    "* [ASZ] Wprowadzenie modu≈Çu pandas\n",
    "* [ASZ] Wizualizacje przy wykorzystaniu matplotlib oraz seaborn\n",
    "* [TGA] Budowa modelu klasyfikatora przy wykorzystaniu modu≈Çu Spark ML.\n",
    "\n",
    "Praca wy≈ÇƒÖcznie na ≈õrodowisku Google Kubernetes Engine. Zak≈Çadamy podstawowƒÖ znajomo≈õƒá jƒôzyka SQL.\n",
    "\n",
    "üôè Podziƒôkowania dla Operatora Chmury Krajowej (https://chmurakrajowa.pl/) i Google Poland za udostƒôpnienie infrastruktury i pokrycie koszt√≥w wykorzystania zasob√≥w podczas zajƒôƒá. üôè\n",
    "\n",
    "\n",
    "## Kubernetes\n",
    "\n",
    "Kubernetes (k8s) to otwarta platforma do koordynacji wysoko dostƒôpnego klastra. \n",
    "* Umo≈ºliwia  deklaratywnƒÖ konfiguracjƒô, automatyzacjƒô wdra≈ºania, skalowanie i autoskalowanie rozwiƒÖza≈Ñ.\n",
    "* Pozwala uruchamiaƒá aplikacje/narzƒôdzie bez przypisywania ich do konkretnej maszyny. Aplikacje muszƒÖ byƒá niezale≈ºne od konkretnego serwera: muszƒÖ byƒá skonteneryzowane.\n",
    "* Mo≈ºliwe do uruchomienia na prywatnym centrum danych, infrastrukturze hybrydowej lub chmurze publicznej.\n",
    "\n",
    "\n",
    "**Klaster Kubernetes**  \n",
    "* **Wƒôze≈Ç sterujƒÖcy (Master node)** koordynuje dzia≈Çanie klastra np. zlecanie uruchomienia aplikacji, utrzymywanie po≈ºƒÖdanego stanu aplikacji, skalowanie aplikacji i instalowanie nowych wersji\n",
    "* **Wƒôze≈Ç roboczy (Worker node)** Na wƒôz≈Çach uruchamiane sƒÖ aplikacje. Na ka≈ºdym wƒô≈∫le dzia≈Ça agent zarzƒÖdzajƒÖcy tym wƒôz≈Çem i komunikujƒÖcy siƒô z masterem Kubernetes (API). Wƒôze≈Ç zawiera tak≈ºe narzƒôdzia do obs≈Çugi kontener√≥w\n",
    "\n",
    "**Pod** to grupa jednego/wielu kontener√≥w wraz ze wsp√≥lnymi zasobami (np. dysk). Pod tworzy \"wirtualney serwer\" i mo≈ºe zawieraƒá r√≥≈ºne kontenery aplikacji wsp√≥≈ÇdzielƒÖce zasoby i kontekst wykonawczy na tym samym wƒô≈∫le.\n",
    "* Pod jest uruchamiany na wƒô≈∫le roboczym. Wƒôze≈Ç jest maszynƒÖ roboczƒÖ, fizycznƒÖ lub wirtualnƒÖ, w zale≈ºno≈õci od klastra. \n",
    "* Wƒôze≈Ç mo≈ºe zawieraƒá wiele pod√≥w. \n",
    "* Kubernetes master automatycznie zleca uruchomienie pod√≥w na r√≥≈ºnych wƒôz≈Çach w ramach klastra. Automatyczne zlecanie uruchomienia bierze pod uwagƒô zasoby dostƒôpne na ka≈ºdym z wƒôz≈Ç√≥w.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/gke.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie danych\n",
    "\n",
    "Bedziemy wykorzystywaƒá danye z ankiety StackOverflow z 2020.\n",
    "\n",
    "https://insights.stackoverflow.com/survey/\n",
    "\n",
    "Dane sa dostepne na google drive. Skorzystamy z modu≈Çu GoogleDriveDownloader, ktory pozwala pobrac dokument o podanym id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path_dir = str(Path.home()) + \"/data/2020/\"  # ustawmy sciezke na HOME/data/2020\n",
    "archive_dir = path_dir + \"survey.zip\"        # plik zapiszemy pod nazwa survey.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sciagniecie pliku we wskazane miejsce\n",
    "gdd.download_file_from_google_drive(file_id='1dfGerWeWkcyQ9GX9x20rdSGj7WtEpzBB',\n",
    "                                    dest_path=archive_dir,\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 1 </span>\n",
    "Zapoznaj sie z plikami tekstowymi (survey_results_public.csv oraz survey_results_schema.csv). Podejrzyj ich zawarto≈õƒá, sprawd≈∫ ich wielko≈õƒá (liczba liniii oraz rozmiar). Wgraj plik do swojego kube≈Çka na GCS do survey/2020/ gs://bdg-lab-$USER/survey/2020/. Jesli nie masz kube≈Çka stw√≥rz go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pod≈ÇƒÖczenie do sesji Spark na GKE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g')\\\n",
    ".config('spark.executor.memory', '1g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostƒôp do danych na GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "print(user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈õcie≈ºka dostƒôpu do pliku na GCS\n",
    "gs_path = f'gs://bdg-lab-{user_name}/survey/2020/survey_results_public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "Platforma Apache Spark posiada komponent Spark SQL, kt√≥ry pozwala traktowaƒá dane jak tabele w bazie danych. Mo≈ºna zak≈Çadaƒá swoje schematy baz danych oraz korzystaƒá z jƒôzyka SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show() # jakie sƒÖ schematy baz danych (logicznie zgrupowane zestawy tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = user_name.replace('-','_')  \n",
    "db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')  # jesli taki schemat istnial to go usun\n",
    "spark.sql(f'CREATE DATABASE {db_name}')                  # stworz taki schemat \n",
    "spark.sql(f'USE {db_name}')                              # wykorzystaj go jako domyslny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show() # jakie sƒÖ schematy baz danych (logicznie zgrupowane zestawy tabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"survey_2020\"                               # nazwa tabeli ktora bedziemy chcieli stworzyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')       # usun te tabele jesli istniala wczesniej \n",
    "\n",
    "# stworz tabele korzystajac z danych we wskazanej lokalizacji\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true) \\\n",
    "          LOCATION \"{gs_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weryfikacja danych \n",
    "Sprawdzmy zaczytane dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"describe {table_name}\").show() # nie wszystkie dane ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"describe {table_name}\").show(100, truncate=False) # niepoprawne typy danych... \"NA\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obs≈Çuga wartosci 'NA' - ponowne stworzenie tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "# wykorzystujemy dodatkowƒÖ opcjƒô: NULLVALUE \"NA\"\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DESCRIBE {table_name}\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT  Age FROM {table_name} ORDER BY Age DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzenie liczno≈õci tabeli\n",
    "spark.sql(f\"select count(*) from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select count(*) from {table_name}\").explain()  # tak jak na poprzednich zajeciach mozemy wygenerowac plany wykonania polecenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select * from {table_name}\")  # mozemy tez w zapytaniu SELECT pobrac wszystkie wiersze i kolumny i zapisac je jako DF (jak na poprzednich zajeciach)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PodglƒÖd danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteka Pandas\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Modu≈Ç Pandas jest bibliotekƒÖ PythonowƒÖ do manipulacji danymi. W szczegolnosci w pandas mozemy stworzyc ramki danych i wykonywac na niej analize, agregacje oraz wizualizacje danych. \n",
    "Przy nieduzych zbiorach danych i prostych operacjach to doskona≈Ça biblioteka. Jednak kiedy zbior danych sie rozrasta lub kiedy wymagane sa zlozone transformacje to operacje moga byc wolne.\n",
    "\n",
    "Operacje na rozproszonych danych sa szybsze. Ale tu takze napotykamy ograniczenia np trudno≈õƒá¬†w wizualizacji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_name} limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wa≈ºne** Metoda toPandas() na ramce pyspark, konwertuje ramkƒô pyspark do ramki pandas. Wykonuje akcje pobrania wszystkich danych z executor√≥w (z JVM) i transfer do  programu sterujacego (driver) i konwersje do typu Pythonowego w notatniku. Ze wzglƒôdu na ograniczenia pamiƒôciowe w programie sterujƒÖcym nale≈ºy to wykonywaƒá na podzbiorach danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = spark.sql(f\"select * from {table_name} LIMIT 10\")\n",
    "local_df = spark.sql(f\"select * from {table_name} LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist_df)  # dataframe Sparkowy (\"przepis na dane, rozproszony, leniwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(local_df)  # dataframe Pandasowy (lokalny, sciƒÖgniƒôty do pamiƒôci operacyjnej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)    # pokazuj wszystkie kolumny\n",
    "# pd.reset_option(‚Äúmax_columns‚Äù)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 2 </span>\n",
    "Napisz w Spark SQL zapytanie kt√≥re zwr√≥ci ≈õredniƒÖ liczbƒô godzin przepracowywanych przez z respondent√≥w pogrupowanych ze wzglƒôdu na kraj. Nastƒôpnie przekszta≈Çƒá wynik do ramki pandasowej i jƒÖ wy≈õwietl.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacje\n",
    "\n",
    "Do wizualizacji bƒôdziemy siƒô pos≈Çugiwaƒá modu≈Çami matplotlib (https://matplotlib.org/) i seaborn (https://seaborn.pydata.org/). Do bardzo rozbudowane modu≈Çy, zachƒôcamy do eksploracji oficjalnych dokumentacji. Na zajƒôciach zrealizujemy nastƒôpujƒÖce wykresy:\n",
    "* histogramy\n",
    "* liniowe \n",
    "* wiolinowe\n",
    "* ko≈Çowe \n",
    "\n",
    "Modu≈Çy wizualizacyjne wymagajƒÖ danych na lokalnej maszynie. MogƒÖ byƒá to natywne typy danych Pythonowe (s≈Çowniki, listy) ale tak≈ºe np ramki danych pandasowe. ~~Nie dzia≈Ça wizualizacja na ramkach danych Sparkowych.~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj histogram wieku respondent√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych\n",
    "# przycinamy dane tylko do zakresu ktory jest potrzebny do realizacji polecenia\n",
    "ages = spark.sql(f\"SELECT CAST (Age AS INT) \\\n",
    "                    FROM {table_name} \\\n",
    "                    WHERE age IS NOT NULL \\\n",
    "                    AND age BETWEEN 10 AND 80\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages.hist(\"Age\", bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(ages, bins=10, rug=True, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaki jest udzia≈Ç programist√≥w hobbist√≥w? Narysuj wykres ko≈Çowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bƒôdzie nas interesowa≈Ça ta proporcja ze wzglƒôdu na p≈Çeƒá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie i zliczenie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "hobby_all = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL GROUP BY Hobbyist\").toPandas()\n",
    "hobby_men = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Man' GROUP BY Hobbyist\").toPandas()\n",
    "hobby_women = spark.sql(f\"SELECT Hobbyist, COUNT(*) AS cnt FROM {table_name} WHERE Hobbyist IS NOT NULL AND Gender='Woman' GROUP BY Hobbyist\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hobby_women.plot.pie(y='cnt', labels=hobby_women['Hobbyist'], title=\"Women\", autopct='%.0f')\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "hobby_all.plot.pie(y='cnt', labels=hobby_all['Hobbyist'], title=\"All\", ax=axes[0], autopct='%.0f')\n",
    "hobby_men.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Men\", ax=axes[1], autopct='%.0f')\n",
    "hobby_women.plot.pie(y='cnt', labels=hobby_men['Hobbyist'], title=\"Women\", ax=axes[2], autopct='%.0f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres liniowy. Zale≈ºno≈õƒá miƒôdzy wiekiem a liczbƒÖ przepracowanych godzin\n",
    "InteresujƒÖ nas dla developerzy profesjonali≈õci (nie hobbi≈õci) w przedziale wiekowym 18-65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "age_work = spark.sql(f\"SELECT age, CAST (avg(WorkWeekHrs) AS INT) AS avg FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs IS NOT NULL AND age BETWEEN 18 AND 65 AND hobbyist = 'No' \\\n",
    "            GROUP BY age \\\n",
    "            ORDER BY age ASC\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_work.plot(x='age', y='avg', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=\"age\", y=\"avg\", kind=\"line\", data=age_work);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres s≈Çupkowy. Poka≈º liczbƒô respondent√≥w na kraj\n",
    "\n",
    "Interesuje nas tylko 10 kraj√≥w o najwy≈ºszej liczbie respondentow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (grupowanie, zliczenie, sortowanie oraz przyciecie do 10 wynik√≥w) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "max_countries = spark.sql(f\"SELECT country, COUNT(*) AS cnt \\\n",
    "                FROM {table_name} \\\n",
    "                GROUP BY country \\\n",
    "                ORDER BY cnt DESC \\\n",
    "                LIMIT 10 \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_countries.plot.bar(y='cnt', x='country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"cnt\", kind=\"bar\",\\\n",
    "            data=max_countries).set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykres s≈Çupkowy. ≈örednie zarobki w  krajach w ktorych jest powyzej 1000 respondent√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie (filtrowanie, grupowanie, wyliczenie sredniej, filtrowanie po liczno≈õci i grup oraz sortowanie) danych na rozproszonych danych (spark sql)\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_salary = spark.sql(f\"SELECT country, \\\n",
    "    CAST (avg(ConvertedComp) AS INT) as avg \\\n",
    "    FROM {table_name} \\\n",
    "    WHERE country IS NOT NULL \\\n",
    "    GROUP BY country \\\n",
    "    HAVING COUNT(*) > 1000 \\\n",
    "    ORDER BY avg DESC \").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_salary.plot.barh((\"country\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot. Pokaz rozklad pensji w krajach gdzie jest powyzej 1000 respondent√≥w\n",
    "Tutaj bƒôdziemy musieli skorzystaƒá z podzapytania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie danych na rozproszonych danych (spark sql). Mamy tu do czynienia z podzapytaniem\n",
    "# pozniej pobranie do pandasowej ramki\n",
    "\n",
    "country_comp = spark.sql(f\"SELECT country, CAST(ConvertedComp AS INT) \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE country IN (SELECT country FROM {table_name} GROUP BY country HAVING COUNT(*) > 1000) \\\n",
    "                AND ConvertedComp IS NOT NULL AND ConvertedComp > 0 \\\n",
    "                ORDER BY ConvertedComp desc\").toPandas()\n",
    "country_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_comp.boxplot(column=\"ConvertedComp\", by=\"country\", \\\n",
    "                     showfliers=False, rot=60, meanline=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"country\", y=\"ConvertedComp\", kind=\"box\", \\\n",
    "            showfliers=False, data=country_comp, palette=\"Blues\")\\\n",
    "    .set_xticklabels(rotation=65)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 3 </span>\n",
    "Narysuj rozklad pensji w zaleznosci od plci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow programowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jƒôzyki programowania sƒÖ zapisane w pojedynczej kom√≥rce. Bƒôdzie trzeba je rozdzieliƒá i zliczyƒá. Tak przygotowane dane dopiero pos≈Çu≈ºƒÖ nam do narysowania wykresu. Wykorzystamy funkcjƒô `posexplode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = spark.sql(f\"select LanguageWorkedWith from {table_name} where LanguageWorkedWith IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_pd = langs.select(\n",
    "        posexplode(split(\"LanguageWorkedWith\", \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\").toPandas()\n",
    "langs_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=langs_pd[\"count\"], y=langs_pd[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres popularnosci jezykow wsrod Data Scientists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdefiniujmy sobie funkcjƒô, kt√≥ra przekszta≈Çca nam jƒôzyki do wymaganej przez nas postaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def prepare_lang(df, colName='LanguageWorkedWith'):\n",
    "    summary = df.select(posexplode(split(colName, \";\")).alias(\"pos\", \"language\")).groupBy(\"language\").count().orderBy(\"count\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_ds = spark.sql(f\"SELECT LanguageWorkedWith \\\n",
    "                FROM {table_name} \\\n",
    "                WHERE DevType LIKE '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(langs_ds).toPandas()\n",
    "\n",
    "figure(figsize=(8, 9))\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres kt√≥rego chcƒÖ wykorzystywaƒá w przysz≈Ço≈õci Data Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_desired = spark.sql(f\"select LanguageDesireNextYear \\\n",
    "                from {table_name} \\\n",
    "                where DevType like '%Data scientist%'\")\n",
    "\n",
    "sum_lang = prepare_lang(lang_desired, 'LanguageDesireNextYear').toPandas()\n",
    "\n",
    "figure(num=None, figsize=(8, 9), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.barh(width=sum_lang[\"count\"], y=sum_lang[\"language\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres prezentujƒÖcy liczbƒô godzin na pracy w zale≈ºno≈õci od wykszta≈Çcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select distinct EdLevel from {table_name}\").show(truncate=False) # jakie sƒÖ warto≈õci wykszta≈Çcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ed_pandas = spark.sql(f\"SELECT EdLevel, WorkWeekHrs FROM {table_name} \\\n",
    "            WHERE WorkWeekHrs BETWEEN 10 AND 80 \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Bachelor‚Äôs degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Master‚Äôs degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pandas['EdLevel'] = ed_pandas['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"WorkWeekHrs\", data=ed_pandas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narysuj wykres wiolinowy pokazujacy rozk≈Çad dochod√≥w w zale≈ºno≈õci od wykszta≈Çcenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_pay = spark.sql(f\"SELECT EdLevel, CAST (CompTotal AS INT) AS CompTotal FROM {table_name} \\\n",
    "            WHERE CompTotal BETWEEN 0 AND 1000000  \\\n",
    "            AND (EdLevel LIKE '%Bachelor%' OR EdLevel LIKE '%Master%' OR EdLevel LIKE '%Other doctoral%')\").toPandas()\n",
    "\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Bachelor‚Äôs degree (B.A., B.S., B.Eng., etc.)','Bachelor')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Master‚Äôs degree (M.A., M.S., M.Eng., MBA, etc.)','Master')\n",
    "ed_pay['EdLevel'] = ed_pay['EdLevel'].replace('Other doctoral degree (Ph.D., Ed.D., etc.)','Doctor')\n",
    "\n",
    "ed_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"EdLevel\", y=\"CompTotal\", kind=\"boxen\",\n",
    "            data=ed_pay);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚≠ê Narysuj heatmape odwiedzin na StackOverflow dla wybranych kraj√≥w ‚≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT DISTINCT SOVisitFreq FROM {table_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_v = spark.sql(f\"SELECT SOVisitFreq, t1.country, COUNT(*)/first(t2.t) AS cnt from {table_name} t1 \\\n",
    "            JOIN (SELECT country, COUNT(*) as t FROM {table_name} GROUP BY country) t2 \\\n",
    "            ON t1.country = t2.country \\\n",
    "            WHERE t1.country IS NOT NULL AND SOVisitFreq IS NOT NULL \\\n",
    "            AND t1.country IN ('Poland', 'United States', 'Russian Federation', 'China', 'India', 'Germany', 'Japan') \\\n",
    "            GROUP BY t1.country, SOVisitFreq\").toPandas()\n",
    "\n",
    "so_v['SOVisitFreq'] = pd.Categorical(so_v['SOVisitFreq'], [\"I have never visited Stack Overflow (before today)\", \"Less than once per month or monthly\", \"A few times per month or weekly\", \"A few times per week\", \"Daily or almost daily\", \"Multiple times per day\"])\n",
    "# so_v.sort_values['SOVisitFreq']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap2_data = pd.pivot_table(so_v, values='cnt', index=['country'], columns='SOVisitFreq')\n",
    "sns.heatmap(heatmap2_data, cmap=\"BuGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE 4 </span>\n",
    "* Narysuj wykres s≈Çupkowy popularno≈õci wykorzystywanych baz danych przez profesjonalnych programist√≥w.\n",
    "* Narysuj wykres ko≈Çowy przedstawiajƒÖcy procentowy udzia≈Ç poziomu wykszta≈Çcenia inz, mgr i dr w grupie respondent√≥w."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
